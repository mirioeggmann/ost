{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Natural Language Processing (NLP)\n",
    "\n",
    "***Summary***\n",
    "- [Load Data](#load-data) <br>\n",
    "- [Preprocessing](#preprocessing) <br>\n",
    "- [Feature Extraction](#feature-extraction) <br>\n",
    "- [Clustering](#clustering) <br>\n",
    "\n",
    "In this Jupyter Notebook you will apply state-of-the-art NLP methods to 357 media releases of the municipality St. Gallen.\n",
    "The corresponding dataset can be found [here](https://daten.stadt.sg.ch/explore/dataset/newsfeed-stadtverwaltung-stgallen/table/?sort=published).\n",
    "Currently, the dataset comprises over 300 HTML files which are neither structured nor assigned to consistent categories.<br><br>\n",
    "\n",
    "Our aim is to group the texts into clusters which are related in content.\n",
    "For this purpose we will clean the raw data and extract embedding vectors for each news release, utilizing a cutting-edge pretrained neural network (transformer).\n",
    "This embedding vector is reduced in dimensionality by applying a modern manifold learning technique.\n",
    "Finally we will cluster these vectors and analyse the quality of these clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --upgrade pip\n",
    "!pip install --user sentence-transformers==2.2.2\n",
    "!pip install --user protobuf==3.19.4\n",
    "!pip install --user umap-learn==0.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load-data'></a>\n",
    "## I. Load Data\n",
    "We load the data into a pandas dataframe.\n",
    "Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, which is often used in machine learning to preprocess raw data.\n",
    "After loading the data, we sort the samples by the date it was published and print the first five samples (`head()`)<br><br>\n",
    "The relevant information is in the columns `Title` and `Text`.\n",
    "However, the `Text` is provided in raw form and contains many HTML tokens, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/newsfeed-stadtverwaltung-stgallen.csv'\n",
    "df = pd.read_csv(path, sep=';')\n",
    "df.sort_values(by=['Veröffentlicht'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0,'Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='preprocessing'></a>\n",
    "## II. Preprocessing\n",
    "In a first step, we will get rid the HTML tokens and useless characters by applying `BeautifulSoup`.\n",
    "If you want more information on how to use `BeautifulSoup`, see [here](https://stackabuse.com/guide-to-parsing-html-with-beautifulsoup-in-python/).\n",
    "The corresponding documentation can be found [here](https://beautiful-soup-4.readthedocs.io/en/latest/).<br>\n",
    "\n",
    "In a second step we will create a new dataframe `df_subset_1`, which only contains a subset of the original dataframe (`Title` and `Text`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "soup = BeautifulSoup(df.loc[0,'Text'], 'html.parser')\n",
    "re.sub(r'\\xa0+', '', soup.text.replace('\\n', ''), flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text().replace('\\n', ''))\n",
    "df['Text'] = df['Text'].apply(lambda x: re.sub(r'\\xa0+', '', x, flags=re.MULTILINE))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_1 = df.drop(['Link','Veröffentlicht','bild_url','bild'], axis=1)\n",
    "df_subset_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we concatenate the `Title` and `Text` into a single column, which then forms the input to our machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_2 = pd.Series(df_subset_1['Title'] + '. ' + df_subset_1['Text'], name='Text').to_frame()\n",
    "df_subset_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature-extraction'></a>\n",
    "## III. Feature Extraction\n",
    "In a next step, we will use a sentence transformer which was pretrained on a large corpus.\n",
    "A sentence transformer is a neural network which was trained to calculate an embedding vector from a sentence / text.\n",
    "These Embedding vectors represent the content of a text.\n",
    "In other words, texts which have a similar meaning result in similar embedding vectors, whereas texts which are different in content result in different embedding vectors.<br><br>\n",
    "For more information on sentence transformer, see [here](https://arxiv.org/abs/1908.10084).<br>\n",
    "For more information on embedding vectors, see [here](https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4).<br>\n",
    "For more information on transformer, see [here](https://towardsdatascience.com/transformers-89034557de14).\n",
    "\n",
    "Fortunately, there exist neural sentence transformers that have been trained on German corpora.\n",
    "Thus, we can skip the tedious work of German-English translation and feed it directly with the German texts (no further preprocessing needed).<br>\n",
    "\n",
    "The transformer is downloaded from [huggingface](https://huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings = model.encode(df_subset_2['Text'], normalize_embeddings=True)\n",
    "corpus_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, each news release is now represented by a 768-dimensional embedding vector.\n",
    "It is generally not recommended working with vectors of such high dimension (remember the \"curse of dimensionality\" from lecture 4), especially if the distances between the vectors are relevant for the clustering algorithm.\n",
    "Thus, we will apply a dimensionality reduction method to reduce the number of dimensions to 50.<br><br>\n",
    "For this purpose we will use a state-of-the-art manifold learning technique called `Uniform Manifold Approximation and Projection` (UMAP).\n",
    "UMAP arranges the samples in the 50-dimensional space in such a way that the arrangement (distances and densities) between the samples of the 768-dimensional space is approximated.\n",
    "This is achieved by an iterative minimization of a cost function.<br><br>\n",
    "[Here](https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668) you can find an more detailed explanation of how UMAP works.<br>\n",
    "[Here](https://arxiv.org/abs/1802.03426) you can find the UMAP paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "n_components = 50\n",
    "\n",
    "reducer = umap.UMAP(n_components=n_components)\n",
    "embedding_reduced = reducer.fit_transform(corpus_embeddings)\n",
    "embedding_plot = umap.UMAP(n_components=2).fit_transform(corpus_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clustering'></a>\n",
    "## IV. Clustering\n",
    "As a final processing step, we will group the texts (more accurately, the extracted embedding vectors) into different clusters.\n",
    "Texts (embedding vectors) that are in the same group should have similar properties, while texts (embedding vectors) in different groups should have highly dissimilar properties.\n",
    "Clustering belongs to the category of unsupervised machine learning and is therefore very difficult to evaluate.\n",
    "There exist many different clustering methods (see [here](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68) for a theoretical explanation):\n",
    "- [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "- [AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\n",
    "- [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)\n",
    "\n",
    "In this example we will apply KMeans to the embedding vectors, to cluster the texts into four groups (the number of groups was chosen arbitrarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "n_clusters = 4\n",
    "\n",
    "km = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "cluster_assignment = km.fit_predict(embedding_reduced)\n",
    "\n",
    "silhouette_avg = silhouette_score(embedding_reduced, cluster_assignment, metric='euclidean')\n",
    "print('Silhouette Coefficient: {:0.3f}'.format(silhouette_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following illustration shows the four clusters, whereas the embedding vectors were reduced to two dimensions (just for the sake of this plot).\n",
    "Note that the clustering algorithm was applied to the 50-dimensional data.\n",
    "That is why the cluster boundaries overlap in some places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "ax.scatter(embedding_plot[:,0],embedding_plot[:,1], s=50, c=cm.nipy_spectral(np.float64(km.labels_) / n_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally very difficult to assess the performance of an unsupervised learning procedure because, by definition, the data do no include any ground truth to which the prediction could be compared.\n",
    "However, there are some scores which can indicate the cluster quality.\n",
    "One of them ist the silhouette plot / silhouette score.<br><br>\n",
    "With the silhouette plot we first calculate a silhouette value for each data sample, which is a measure of how similar a sample is to its own cluster (cohesion) compared to other clusters (separation).\n",
    "It ranges from -1 to +1, with -1 representing poor cohesion / separation and +1 representing good cohesion / separation.\n",
    "If all these silhouette values are sorted by value, visualized as bar plot and colorized according to the cluster assignment, we get the silhouette plot (see below).\n",
    "If all the bars have about the same length (positive values) then the clustering algorithm was able to find distinct clusters.\n",
    "The average over all silhouette values corresponds to the silhouette score.<br><br>\n",
    "More information on the silhouette method can be found [here](https://en.wikipedia.org/wiki/Silhouette_(clustering)).<br>\n",
    "The following code was taken from [here](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "fig.set_size_inches(18, 7)\n",
    "\n",
    "ax.set_xlim([-0.1, 1])\n",
    "ax.set_ylim([0, len(embedding_reduced) + (n_clusters + 1) * 10])\n",
    "\n",
    "# Compute the silhouette scores for each sample\n",
    "sample_silhouette_values = silhouette_samples(embedding_reduced, cluster_assignment, metric='euclidean')\n",
    "\n",
    "y_lower = 10\n",
    "for i in range(n_clusters):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = \\\n",
    "        sample_silhouette_values[cluster_assignment == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "    ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                        0, ith_cluster_silhouette_values,\n",
    "                        facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "ax.set_title(\"The silhouette plot for the various clusters.\")\n",
    "ax.set_xlabel(\"The silhouette coefficient values\")\n",
    "ax.set_ylabel(\"Cluster label\")\n",
    "\n",
    "# The vertical line for average silhouette score of all the values\n",
    "ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "ax.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the evaluation we print the title of four news releases for each cluster.\n",
    "Can you find the similarities / dissimilarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "\n",
    "doc_class = {}\n",
    "\n",
    "for class_label in np.unique(km.labels_):\n",
    "    embedding_subset = embedding_reduced[km.labels_==class_label]\n",
    "    df_subset = df_subset_1[km.labels_==class_label]\n",
    "\n",
    "    cluster_center = km.cluster_centers_[np.newaxis,class_label]\n",
    "\n",
    "    metrics = euclidean_distances(embedding_subset, cluster_center)\n",
    "    idx_sample = np.argsort(metrics[:,0])\n",
    "\n",
    "    doc_class[class_label] = df_subset.iloc[idx_sample, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_label, texts in doc_class.items():\n",
    "    print('Class {:d}'.format(class_label))\n",
    "    for text in texts[:4]:\n",
    "        print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "747131c1b9b3e4ac9b17fc6ae0b784c32121a8c91b4324a16e14e5d85f0a17bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
