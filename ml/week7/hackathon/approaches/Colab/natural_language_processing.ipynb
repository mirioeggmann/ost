{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEKFA6oc0snn"
      },
      "source": [
        "# Introduction Natural Language Processing (NLP)\n",
        "\n",
        "***Summary***\n",
        "- [Load Data](#load-data) <br>\n",
        "- [Preprocessing](#preprocessing) <br>\n",
        "- [Feature Extraction](#feature-extraction) <br>\n",
        "- [Clustering](#clustering) <br>\n",
        "\n",
        "In this Jupyter Notebook you will apply conventional NLP methods to 323 media releases of the St. Gallen City Police.\n",
        "The corresponding dataset can be found [here](https://daten.stadt.sg.ch/explore/dataset/newsfeed-stadtpolizei-stgallen-medienmitteilungen/table/?sort=published).\n",
        "Currently, the dataset comprises over 300 HTML files neither structured nor assigned to consistent categories.<br><br>\n",
        "\n",
        "Our aim is to group the texts into clusters which are related in content.\n",
        "To this end, we will first clean the raw data, translate it into English, and reduce the texts to expressive words in their root form.<br>\n",
        "Next, a feature vector is extracted for each document which should represent the content of the document.\n",
        "Finally we will cluster these vectors and analyse the quality of these clusters.<br><br>\n",
        "\n",
        "Parts of this Jupyter Notebook were copied from [this tutorial](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oC2P66O0snp"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOoClO3r0snq"
      },
      "source": [
        "<a id='load-data'></a>\n",
        "## I. Load Data\n",
        "We load the data into a pandas dataframe.\n",
        "Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, which is often used in machine learning to preprocess raw data.<br><br>\n",
        "The relevant information is in the columns `Title` and `Text`.\n",
        "However, the `Text` is provided in raw form and contains many HTML tokens, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload newsfeed-stadtpolizei-stgallen-medienmitteilungen.csv as soon as `Choose Files` button appears\n",
        "uploaded_data = files.upload()\n",
        "df = pd.read_csv(io.BytesIO(uploaded_data['newsfeed-stadtpolizei-stgallen-medienmitteilungen.csv']), sep=';')"
      ],
      "metadata": {
        "id": "GjcP-8mV_ram"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXWSFft-0snq"
      },
      "outputs": [],
      "source": [
        "df.sort_values(by=['Veröffentlicht'], inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkXKYxvk0snq"
      },
      "outputs": [],
      "source": [
        "df.loc[0,'Text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6lqiFSx0snq"
      },
      "source": [
        "<a id='preprocessing'></a>\n",
        "## II. Preprocessing\n",
        "In a first step, we will get rid the HTML tokens and useless characters by applying `BeautifulSoup`.\n",
        "If you want more information on how to use `BeautifulSoup`, see [here](https://stackabuse.com/guide-to-parsing-html-with-beautifulsoup-in-python/).\n",
        "The corresponding documentation can be found [here](https://beautiful-soup-4.readthedocs.io/en/latest/).<br>\n",
        "\n",
        "In a second step we will create a new dataframe `df_subset_1`, which only contains a subset of the original dataframe (`Title` and `Text`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI6LbFrN0snr"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(df.loc[0,'Text'], 'html.parser')\n",
        "soup.text.replace('\\n', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGhYTxQS0snr"
      },
      "outputs": [],
      "source": [
        "df['Text'] = df['Text'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text().replace('\\n', ''))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLtnIOgB0snr"
      },
      "outputs": [],
      "source": [
        "df_subset_1 = df.drop(['Link','Veröffentlicht','Bild URL','Bild'], axis=1)\n",
        "df_subset_1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fznTGBIz0snr"
      },
      "source": [
        "Since most existing NLP tools only work with English words, we first need to translate the German texts.\n",
        "For this purpose we use the Google Translate service which is accessed by using the Python package `deep-translator`. <br><br>\n",
        "Since Google blocks us if the translation service is called too often, the texts are translated in batches and we have to wait 2 seconds after each translation step.\n",
        "Thus, the translation of the entire dataset takes quite a long time (~30min).\n",
        "For this reason I have provided you the translated dataframe in `df_subset_1_en`, which you can load by executing the next cell."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-translator"
      ],
      "metadata": {
        "id": "CoJhSVdxAPJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sGaSXwp0snr"
      },
      "outputs": [],
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "gtr = GoogleTranslator(source='de', target='en')\n",
        "\n",
        "translated_title = gtr.translate_batch(df_subset_1['Title'].tolist())\n",
        "translated_text  = gtr.translate_batch(df_subset_1['Text'].tolist())\n",
        "\n",
        "df_subset_1['Title'] = translated_title\n",
        "df_subset_1['Text']  = translated_text\n",
        "\n",
        "df_subset_1.tail()\n",
        "df_subset_1.to_csv('../data/df_subset_1_en.csv', sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload df_subset_1_en.csv as soon as `Choose Files` button appears\n",
        "uploaded_data = files.upload()\n",
        "df_subset_1_en = pd.read_csv(io.BytesIO(uploaded_data['df_subset_1_en.csv']), sep=';')\n",
        "df_subset_1_en.head()"
      ],
      "metadata": {
        "id": "hIzVZWhGGiN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRXRybVK0sns"
      },
      "source": [
        "Next, we concatenate the `Title` and `Text` into a single column, which then forms the input to our machine learning algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjgKvgrE0sns"
      },
      "outputs": [],
      "source": [
        "df_subset_2 = pd.Series(df_subset_1_en['Title'] + ' ' + df_subset_1_en['Text'], name='Text').to_frame()\n",
        "df_subset_2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQCDfSvi0sns"
      },
      "source": [
        "There are some common preprocessing steps that are applied before training a machine learning model.\n",
        "Their purpose is to standardize the documents and reduce the number of words.\n",
        "\n",
        "- Tokenize, i.e. split texts into words ([I love NLP] → [I, love, NLP])\n",
        "- Expand contractions (I'm → I am)\n",
        "- Lowercase all words\n",
        "- Remove stopwords (e.g. common words like the, a, and etc. because they have no expressive meaning)\n",
        "- Keep specific word forms only (e.g. all nouns and adjectives)\n",
        "- Lemmatization, i.e. reduces inflected words and ensures that the root word is a proper word (am → be, was → be, were → be)\n",
        "- Stemming, i.e. reduces inflected words to their stem (root or base) forms even if the stem itself is not a valid word (happy → happi)\n",
        "\n",
        "I our case we will apply the following preprocessing steps:\n",
        "- Tokenize\n",
        "- Remove Contractions\n",
        "- Keep alphabetic tokens only\n",
        "- Remove English stopwords\n",
        "- Keep only nouns, adjectives and verbs\n",
        "- Lemmatize words\n",
        "\n",
        "We achieve this by using the class TextPreprocessor in the module textPreprocessing.<br>\n",
        "Further information on text preprocessing can be found [here](https://towardsdatascience.com/text-preprocessing-steps-and-universal-pipeline-94233cb6725a)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "1UGmrCI8IGIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Fdh9t5x0sns"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "\n",
        "# Select textProcessing module as soon as `Choose Files` button appears\n",
        "src = list(files.upload().values())[0]\n",
        "open('textPreprocessing.py','wb').write(src)\n",
        "from textPreprocessing import TextPreprocessor\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "language = 'english'\n",
        "stop_words = set(stopwords.words(language))\n",
        "stop_words.add('\\\"')\n",
        "stop_words.add('\\'')\n",
        "stop_words.add('\\'\\'')\n",
        "stop_words.add('`')\n",
        "stop_words.add('``')\n",
        "stop_words.add('\\'s')\n",
        "# Extend the stop_word list if appropriate.\n",
        "\n",
        "processor = TextPreprocessor(\n",
        "    language = language,\n",
        "    pos_tags = {wordnet.ADJ, wordnet.NOUN},\n",
        "    stopwords = stop_words,\n",
        "    n_jobs = 6,\n",
        "    alpha_only=True,\n",
        ")\n",
        "\n",
        "df_subset_2['Processed'] = processor.transform(df_subset_2['Text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDJLlvFV0snt"
      },
      "outputs": [],
      "source": [
        "df_subset_2.loc[0,'Processed']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsUH88PE0snt"
      },
      "source": [
        "<a id='feature-extraction'></a>\n",
        "## III. Feature Extraction\n",
        "As part of the feature extraction process we determine a feature vector for each standardized, cleaned text.\n",
        "This feature vector should represent the relevant content of the text in numbers.\n",
        "For this purpose, we use the so-called `Term Frequency - Inverse Document Frequency` (TF-IDF) method.<br><br>\n",
        "The TF-IDF represents a document by a vector which has an entry for each word in the corpus.\n",
        "It assigns each vector entry the number of occurrence of the corresponding word (Term Frequency), weighted by how often the word occurs in the entire corpus (IDF).\n",
        "For more information on TF-IDF, see [here](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) or [here](https://towardsdatascience.com/how-tf-idf-works-3dbf35e568f0).<br><br>\n",
        "[Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) provides an implementation of the TF-IDF method, which we will use here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVHP4mR00snt"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2)\n",
        "X_tfidf = vectorizer.fit_transform(df_subset_2['Processed'])\n",
        "\n",
        "print('n_samples: {:d}, n_features: {:d}'.format(*X_tfidf.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puUEaeN20snt"
      },
      "outputs": [],
      "source": [
        "list(vectorizer.get_feature_names())[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fma-NEEw0snt"
      },
      "source": [
        "As you can see above, the feature vector which results from applying the TfidfVectorizer to this corpus is of dimension 1457.\n",
        "It is generally not recommended working with vectors of such high dimension (remember the \"curse of dimensionality\" from lecture 4), especially if the distances between the vectors are relevant for the clustering algorithm.\n",
        "Thus, we will apply a dimensionality reduction method to reduce the number of dimensions to 20.<br><br>\n",
        "Truncated Singular Value Decomposition (TruncatedSVD) is a linear dimensionality reduction method which is often used for sparse data (data with many zero entries).\n",
        "It is basically the same as Principal Component Analysis (PCA), but without prior subtraction of the mean vector (which would turn a sparse vector into a dense vector).\n",
        "If you want more information TruncatedSVD, see [here](https://towardsdatascience.com/recommender-system-singular-value-decomposition-svd-truncated-svd-97096338f361).\n",
        "For a comparison between PCA and TruncatedSVD, see [here](https://stats.stackexchange.com/a/342072).<br><br>\n",
        "After reducing the dimensions, the feature vector is normalized, which improves the clustering performance.\n",
        "Both steps (dimensionality reduction and normalization) are combined to a single step, using sklearn's [pipeline idea](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-maHyBh0snt"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "n_components = 20\n",
        "\n",
        "svd = TruncatedSVD(n_components)\n",
        "normalizer = Normalizer(copy=False)\n",
        "lsa = make_pipeline(svd, normalizer)\n",
        "\n",
        "X_svd = lsa.fit_transform(X_tfidf)\n",
        "\n",
        "explained_variance = svd.explained_variance_ratio_.sum()\n",
        "print('Explained variance of the SVD step: {:d}%'.format(int(explained_variance * 100)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDjA4_Vu0snu"
      },
      "outputs": [],
      "source": [
        "X_svd.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmUQX2fw0snu"
      },
      "source": [
        "<a id='clustering'></a>\n",
        "## IV. Clustering\n",
        "As a final processing step, we will group the texts (more accurately, the extracted feature vectors) into different clusters.\n",
        "Texts (feature vectors) that are in the same group should have similar properties, while texts (feature vectors) in different groups should have highly dissimilar properties.\n",
        "Clustering belongs to the category of unsupervised machine learning and is therefore very difficult to evaluate.\n",
        "There exist many different clustering methods (see [here](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68) for a theoretical explanation):\n",
        "- [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
        "- [AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\n",
        "- [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)\n",
        "\n",
        "In this example we will apply KMeans to the feature vectors, to cluster the texts into five groups (the number of groups was chosen arbitrarily)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUZuKN-70snu"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "n_clusters = 5\n",
        "\n",
        "km = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "cluster_labels = km.fit_predict(X_svd)\n",
        "\n",
        "silhouette_avg = silhouette_score(X_svd, cluster_labels, metric='euclidean')\n",
        "print('Silhouette Coefficient: {:0.3f}'.format(silhouette_avg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tbAmHug0snu"
      },
      "source": [
        "It is generally very difficult to assess the performance of an unsupervised learning procedure because, by definition, the data do no include any ground truth to which the prediction could be compared.\n",
        "However, there are some scores which can indicate the cluster quality.\n",
        "One of them ist the silhouette plot / silhouette score.<br><br>\n",
        "With the silhouette plot we first calculate a silhouette value for each data sample, which is a measure of how similar a sample is to its own cluster (cohesion) compared to other clusters (separation).\n",
        "It ranges from -1 to +1, with -1 representing poor cohesion / separation and +1 representing good cohesion / separation.\n",
        "If all these silhouette values are sorted by value, visualized as bar plot and colorized according to the cluster assignment, we get the silhouette plot (see below).\n",
        "If all the bars have about the same length (positive values) then the clustering algorithm was able to find distinct clusters.\n",
        "The average over all silhouette values corresponds to the silhouette score.<br><br>\n",
        "More information on the silhouette method can be found [here](https://en.wikipedia.org/wiki/Silhouette_(clustering)).<br>\n",
        "The following code was taken from [here](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Eyfhe-00snu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.cm as cm\n",
        "\n",
        "fig, ax = plt.subplots(1,1)\n",
        "fig.set_size_inches(18, 7)\n",
        "\n",
        "ax.set_xlim([-0.5, 1])\n",
        "ax.set_ylim([0, len(X_svd) + (n_clusters + 1) * 10])\n",
        "\n",
        "# Compute the silhouette scores for each sample\n",
        "# sample_silhouette_values = silhouette_samples(X_svd, cluster_labels, metric='cosine')\n",
        "sample_silhouette_values = silhouette_samples(X_svd, cluster_labels, metric='euclidean')\n",
        "\n",
        "y_lower = 10\n",
        "for i in range(n_clusters):\n",
        "    # Aggregate the silhouette scores for samples belonging to\n",
        "    # cluster i, and sort them\n",
        "    ith_cluster_silhouette_values = \\\n",
        "        sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "    ith_cluster_silhouette_values.sort()\n",
        "\n",
        "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "    y_upper = y_lower + size_cluster_i\n",
        "\n",
        "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "    ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                        0, ith_cluster_silhouette_values,\n",
        "                        facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "    # Label the silhouette plots with their cluster numbers at the middle\n",
        "    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "    # Compute the new y_lower for next plot\n",
        "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "ax.set_title(\"The silhouette plot for the various clusters.\")\n",
        "ax.set_xlabel(\"The silhouette coefficient values\")\n",
        "ax.set_ylabel(\"Cluster label\")\n",
        "\n",
        "# The vertical line for average silhouette score of all the values\n",
        "ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "ax.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5jC9bNO0snu"
      },
      "source": [
        "Next we want to take a closer look at some results.\n",
        "The following code snippet outputs ten keywords that occur frequently in a document that is near a cluster centroid.<br><br>\n",
        "Moreover, in cell 47 we print some document titles for each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56LArvXu0snv"
      },
      "outputs": [],
      "source": [
        "print(\"Top terms per cluster:\")\n",
        "\n",
        "original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
        "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
        "\n",
        "terms = vectorizer.get_feature_names()\n",
        "for i in range(n_clusters):\n",
        "    print('Cluster {:d}:'.format(i), end='')\n",
        "    for ind in order_centroids[i, :10]:\n",
        "        print(' {:s}'.format(terms[ind]), end='')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1u6BPFs0snv"
      },
      "outputs": [],
      "source": [
        "idx_sample = np.argsort(np.linalg.norm(X_svd[...,np.newaxis] - km.cluster_centers_.T[np.newaxis], axis=1), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X103eVwA0snv"
      },
      "outputs": [],
      "source": [
        "for i in range(n_clusters):\n",
        "    print('Examples cluster {:d}: '.format(i))\n",
        "    for idx in idx_sample[:4,i]:\n",
        "        print(df.loc[idx,'Title'])\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6.9 ('venv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "747131c1b9b3e4ac9b17fc6ae0b784c32121a8c91b4324a16e14e5d85f0a17bd"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}