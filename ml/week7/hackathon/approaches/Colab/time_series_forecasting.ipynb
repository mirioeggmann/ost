{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsuaEjF005BN"
      },
      "source": [
        "# Introduction to Time Series Forecasting\n",
        "\n",
        "***Summary***\n",
        "- [Load data](#load-data) <br>\n",
        "- [Preprocess data](#preprocess-data) <br>\n",
        "- [Prediction](#prediction) <br>\n",
        "- [Forecasting](#forecasting) <br>\n",
        "\n",
        "In this lab we will use the dataset [Verkehrsz√§hlung MIV St. Gallen](https://stadt-stgallen.opendatasoft.com/explore/dataset/verkehrszahlung-miv-stadt-stgallen/information/?disjunctive.ort_id&disjunctive.bezeichnung&disjunctive.ri&disjunctive.ort_richtung_id) provided by [Open Data St. Gallen](https://stadt-stgallen.opendatasoft.com/pages/uber-uns/) to train and evaluate a simple classifier and perform time series forecasting.\n",
        "\n",
        "We will start by loading the data from a csv into a [pandas](https://pandas.pydata.org/) dataframe. Utilizing pandas' rich set of functions we will then preprocess the raw data and prepare it for our purpose. Next, we visualize a subset of the data to identify distinctive properties and gain some inspiration for further processing. Based on this insights, we train a logistic regression model and various time series forecasting models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CblOJgy05BP"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import files\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrWVWE-s05BQ"
      },
      "source": [
        "<a id='load-data'></a>\n",
        "## I. Load Data\n",
        "This dataset can be downloaded as .csv-file under [this link](https://stadt-stgallen.opendatasoft.com/explore/dataset/verkehrszahlung-miv-stadt-stgallen/export/?disjunctive.ort_id&disjunctive.bezeichnung&disjunctive.ri&disjunctive.ort_richtung_id). Download the dataset and modify the following path accordingly.<br>\n",
        "You can use the [head function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) to display the first five rows of the pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload verkehrszahlung-miv-stadt-stgallen.csv as soon as `Choose Files` button appears\n",
        "uploaded_data = files.upload()\n",
        "df = pd.read_csv(io.BytesIO(uploaded_data['verkehrszahlung-miv-stadt-stgallen.csv']), sep=';')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5nJBmHdw4bfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzCL731A05BQ"
      },
      "source": [
        "<a id='preprocess-data'></a>\n",
        "## II. Preprocess data\n",
        "This dataset comprises traffic counting data from 51 different locations for different directions.\n",
        "As part of this preprocessing step, we will select the location/direction with the most data.\n",
        "Then we will further reduce the amount of data and create a `df_predict` and a `df_forecast` that contain only the necessary data for the prediction task and the forecasting task, respectively.<br><br>\n",
        "First, lets determine the number of unique values for each column with the following command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSsJbxqO05BQ"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(df.nunique()).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZceJSTBo05BR"
      },
      "source": [
        "In order to determine the location/direction combination which has the most data samples we use the function [value_counts](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html).<br>\n",
        "The [head(10)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) command displays the first ten rows of the resulting [Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYBOZ9sg05BR"
      },
      "outputs": [],
      "source": [
        "df['ORT-RICHTUNG ID'].value_counts().head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfW1ea-805BR"
      },
      "source": [
        "Next we determine the `ORT-RICHTUNG ID` of the location / direction combination with the most data samples by extracting the [first_valid_index](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.first_valid_index.html) of the sorted pandas series above.<br>\n",
        "Then we extract a subset which only contains data samples of the corresponding `ORT-RICHTUNG ID`.\n",
        "By executing [nunique](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.nunique.html) we can verify that this subset really contains only one value for `STANDORT` and `RICHTUNG`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJCAGoCV05BR"
      },
      "outputs": [],
      "source": [
        "max_count_idx = df['ORT-RICHTUNG ID'].value_counts().first_valid_index()\n",
        "max_count_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zlcb5ACM05BR"
      },
      "outputs": [],
      "source": [
        "df_sub_1 = df.loc[df['ORT-RICHTUNG ID']==max_count_idx]\n",
        "df_sub_1.reset_index(drop=True, inplace=True)\n",
        "df_sub_1.nunique().head(9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aw3MnAq05BR"
      },
      "outputs": [],
      "source": [
        "df_sub_1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPv5xiQO05BS"
      },
      "source": [
        "It turns out that we are analyzing the traffic volume at the `Herisauerstrasse 58`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HjEEBP805BS"
      },
      "outputs": [],
      "source": [
        "street_name = df_sub_1['BEZEICHNUNG'].unique()\n",
        "print(*street_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewHwQRo205BS"
      },
      "source": [
        "By creating a [pivot_table](https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html) we can determine the number of samples for each day of the week."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZ0XPU2S05BS"
      },
      "outputs": [],
      "source": [
        "df_sub_1.pivot_table(index='BEZEICHNUNG',columns='WOCHENTAG', values='ORT-RICHTUNG ID', aggfunc='count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVJso60P05BS"
      },
      "source": [
        "<a id='prediction'></a>\n",
        "## III. Prediction\n",
        "In this chapter we will use the data subset to train and test a very simple (and admittedly not very useful) classifier.\n",
        "To this end, the `TAGESTOTAL` variable is the predictor and the qualitative `ARBEITSTAG` variable is the response which we will use to train a logistic regression model (which was introduced in lecture week 5).<br><br>\n",
        "First, let's create a new dataframe called `df_predict` which contains only the data required for this classification task.\n",
        "Next, we will visualize histogram of the data subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrQWkzbL05BS"
      },
      "outputs": [],
      "source": [
        "df_predict = df_sub_1.loc[:,['ARBEITSTAG','TAGESTOTAL']]\n",
        "df_predict.rename({'ARBEITSTAG':'y','TAGESTOTAL':'x'}, inplace=True, axis=1)\n",
        "df_predict.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVK_NdWT05BT"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(1,1, figsize=(10,5))\n",
        "sns.histplot(data=df_predict, x='x', hue='y', kde=True, palette=sns.color_palette('bright')[:2], ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-LilY6Y05BT"
      },
      "source": [
        "In the following we will:\n",
        "- create a training and test dataset by using sklearn's [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function.\n",
        "- instantiate and fit a [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model using the training split.\n",
        "- evaluate the classifier's performance based on the test split, by using the score method.\n",
        "- determine the decision boundary based on the trained model parameters `coef_` and `intercept_` and display this decision boundary in the histogram plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_40OarM05BT"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_predict['x'], df_predict['y'],\n",
        "    stratify=df_predict['y'], test_size=0.33, random_state=42)\n",
        "\n",
        "\n",
        "classifier = LogisticRegression(penalty='none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Fu0PvSe05BT"
      },
      "outputs": [],
      "source": [
        "# Fit classifier to training data\n",
        "classifier.fit(X_train[:,np.newaxis], y_train)\n",
        "\n",
        "# Score classifier based on test set\n",
        "score = classifier.score(X_test[:,np.newaxis], y_test)\n",
        "print('Mean accuracy: {:4.3f}'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuNh5nOQ05BT"
      },
      "source": [
        "In lecture 5 we've learned that a simple logistic regression model looks as follows:<br><br>\n",
        "$\\log(\\frac{p(X)}{1-p(X)})=\\beta_0 + \\beta_1 \\cdot X$<br><br>\n",
        "During training, the parameters $\\beta_0$ and $\\beta_1$ are adjusted so that the model fits the training data as closely as possible.\n",
        "These fitted parameters $\\beta_0$ and $\\beta_1$ can be accessed by the properties `intercept_` and `coef_`, respectively.\n",
        "Since the decision boundary corresponds to the $X$ value for which\n",
        "$\\log(\\frac{p(X)}{1-p(X)})=0$, the decision boundary can be calculated as follows:<br><br>\n",
        "$X_b = -\\frac{\\beta_0}{\\beta_1}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNaq2Pv305BU"
      },
      "outputs": [],
      "source": [
        "print(classifier.classes_)\n",
        "print(classifier.coef_)\n",
        "print(classifier.intercept_)\n",
        "\n",
        "boundary = -classifier.intercept_ / classifier.coef_[0]\n",
        "\n",
        "print(boundary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAXJC0Gx05BU"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(1,1, figsize=(10,5))\n",
        "sns.histplot(data=df_predict, x='x', hue='y', kde=True, palette=sns.color_palette('bright')[:2], ax=ax)\n",
        "ax.axvline(x=boundary, c='r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgYG8bw-05BU"
      },
      "source": [
        "<a id='forecasting'></a>\n",
        "## IV. Forecasting\n",
        "As part of this section, we will completely rearrange the dataframe `df_tmp_1` to obtain hourly traffic samples and create a `df_forecast` dataframe.\n",
        "Based on this dataframe we'll attempt to make traffic forecasts for future time steps for which the correct values are not known.<br><br>\n",
        "\n",
        "For this purpose we will test three different approaches:\n",
        "- [Exponential Smoothing](https://www.statsmodels.org/stable/generated/statsmodels.tsa.holtwinters.SimpleExpSmoothing.html)\n",
        "- [Triple Exponential Smoothing](https://www.statsmodels.org/dev/generated/statsmodels.tsa.holtwinters.ExponentialSmoothing.html)\n",
        "- [Facebook Prophet](https://facebook.github.io/prophet/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHBLdyAl05BU"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMCxnsu905BU"
      },
      "outputs": [],
      "source": [
        "df_sub_1.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9RdRwUm05BU"
      },
      "source": [
        "As you can see in the cell above, each row of the dataframe `df_sub_1` corresponds to the measurements of one day.\n",
        "However, the traffic data is actually provided on an hourly basis.\n",
        "The columns `1` to `24` contain the data for each hour of the corresponding day.<br>\n",
        "Thus, we will rearrange the dataframe to have the datetime (day and hour) in the first column and the corresponding traffic value in the second column.<br><br>\n",
        "To this end we will repeat each entry of the `DATUM` column 24 times and add the hour obtained by stacking the columns `1` to `24`.\n",
        "The `df_forecast` results from adding these two dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJcNKw3r05BU"
      },
      "outputs": [],
      "source": [
        "df_tmp_1 = df_sub_1['DATUM'].copy()\n",
        "df_tmp_1 = pd.to_datetime(df_tmp_1)\n",
        "df_tmp_1 = df_tmp_1.repeat(24).reset_index(drop=True)\n",
        "df_tmp_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x95aflq105BU"
      },
      "outputs": [],
      "source": [
        "df_tmp_2 = df_sub_1.loc[:,'1':'24'].stack().reset_index().drop('level_0', axis=1)\n",
        "df_tmp_2.rename(columns={'level_1': 'ds', 0: 'y'}, inplace=True)\n",
        "df_tmp_2['ds'] = df_tmp_2['ds'].astype('timedelta64[h]')\n",
        "df_tmp_2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seB3_5vj05BU"
      },
      "outputs": [],
      "source": [
        "df_forecast = df_tmp_2.copy()\n",
        "df_forecast['ds'] = df_tmp_1 + df_tmp_2['ds']\n",
        "df_forecast.sort_values(by=['ds'], inplace=True)\n",
        "df_forecast.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7dAJ8R705BV"
      },
      "source": [
        "Next we plot this time series a by using the pandas' plot function.\n",
        "The traffic development shows a distinctive pattern.\n",
        "On weekends and at night, the traffic volume is significantly lower than at noon on a weekday.<br>\n",
        "Next, we fit a time series model to these data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LORViwjE05BV"
      },
      "outputs": [],
      "source": [
        "df_forecast.iloc[-1500:].plot(x='ds', y='y', title='Traffic at {}'.format(*street_name), xlabel='Time', ylabel='Number of Vehicles', figsize=(30,4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IyCZhrM05BV"
      },
      "source": [
        "### Exponential Smoothing\n",
        "Exponential smoothing is a very simple forecasting method, where the estimated future sample corresponds to the weighted sum of the previous (known) samples.\n",
        "The weight decreases exponentially the further in the past the sample is.<br>\n",
        "The estimated value $s_t$ at time step $t$ is calculated as follows:<br><br>\n",
        "$s_t = \\alpha \\cdot x_t + (1 - \\alpha) \\cdot s_{t-1} \\qquad t > 0$<br><br>\n",
        "For further theoretical information on exponential smoothing, see [here](https://en.wikipedia.org/wiki/Exponential_smoothing).<br><br>\n",
        "In a first step we will use the data of the previous week in the dataset to fit an [SimpleExpSmoothing](https://www.statsmodels.org/stable/generated/statsmodels.tsa.holtwinters.SimpleExpSmoothing.html) model of the python package [statsmodels](https://www.statsmodels.org/stable/index.html).\n",
        "Using this model, we attempt to predict the traffic for the next 24 hours in an one-hour interval.\n",
        "Try to improve the predicted values by adjusting the function attributes.\n",
        "Information on the function API can be found [here](https://www.statsmodels.org/stable/generated/statsmodels.tsa.holtwinters.SimpleExpSmoothing.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjJQwp0k05BV"
      },
      "outputs": [],
      "source": [
        "df_forecast_exp = df_forecast.iloc[-168:].copy()\n",
        "df_forecast_exp = df_forecast_exp.set_index('ds')\n",
        "df_forecast_exp.index = pd.DatetimeIndex(df_forecast_exp.index.values,\n",
        "                                         df_forecast_exp.index.inferred_freq)\n",
        "df_forecast_exp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiF0SczE05BV"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
        "\n",
        "model = SimpleExpSmoothing(df_forecast_exp).fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrkdUiQk05BV"
      },
      "outputs": [],
      "source": [
        "# Forecast 100 weeks into the future\n",
        "pred = model.forecast(24)\n",
        "\n",
        "fig, ax = plt.subplots(1,1,figsize=(10,4))\n",
        "ax.plot(pred.index, pred, c='r')\n",
        "df_forecast_exp.iloc[-168:].plot(ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzOKnjly05BV"
      },
      "source": [
        "### Triple Exponential Smoothing (TES)\n",
        "Triple Exponential Smoothing (TES) (aka Hold-Winters algorithm) is an extension of the Simple Exponential Smoothing model to account for seasonality.\n",
        "As the name already suggests, three components are estimated by using exponential smoothing.\n",
        "Namely, the `level` component, `trend` component, and the `seasonality` component.<br><br>\n",
        "For further theoretical information on TES see [here](https://en.wikipedia.org/wiki/Exponential_smoothing#Triple_exponential_smoothing_(Holt_Winters))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj4pYEvM05BV"
      },
      "outputs": [],
      "source": [
        "df_forecast_trip = df_forecast.iloc[-120:].copy()\n",
        "df_forecast_trip = df_forecast_trip.set_index('ds')\n",
        "df_forecast_trip.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YY91gbjO05BV"
      },
      "outputs": [],
      "source": [
        "df_forecast_trip = df_forecast.iloc[-120:].copy()\n",
        "df_forecast_trip = df_forecast_trip.set_index('ds')\n",
        "df_forecast_trip.index = pd.DatetimeIndex(df_forecast_trip.index.values,\n",
        "                                          df_forecast_trip.index.inferred_freq)\n",
        "df_forecast_trip.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf3KfJnO05BW"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "\n",
        "model = ExponentialSmoothing(df_forecast_trip, seasonal='add', seasonal_periods=24).fit(optimized = True)\n",
        "\n",
        "# Forecast 48 hours out\n",
        "pred = model.forecast(96)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TomD6lP05BW"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,1,figsize=(10,4))\n",
        "ax.plot(pred, c='r')\n",
        "df_forecast_trip.plot(ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvulJjE505BW"
      },
      "source": [
        "## Facebook Prophet\n",
        "\n",
        "Prophet is a forecasting procedure for time series which is available as Python package.\n",
        "It is a state-of-the-art forecasting method which works best with time series that have strong seasonal effects and several seasons of historical data.\n",
        "\n",
        "Unlike most other methods, Prophet frames the forecast problem as a curve-fitting exercise, which is inherently different from time series models that consider the temporal dependency in the data (ARMA / ARIMA / SARIMA).\n",
        "The time series model consists of three decomposable components:<br><br>\n",
        "$y(t) = g(t) + s(t) + h(t) + \\epsilon_t$<br><br>\n",
        "$g(t)$ is the trend function, $s(t)$ represents the periodic changes (e.g. weekly and yearly seasonality) and $h(t)$ represents the effects of holidays.\n",
        "This model can be considered as Generalized Additive Model (GAM), which will be introduced in semester week 10.<br><br>\n",
        "If you want to learn more about GAM's, see 7.7 in the textbook.\n",
        "If you want to learn more about Prophet, see [this paper](https://peerj.com/preprints/3190/).\n",
        "More details on how to apply Prophet in Python can be found [here](https://facebook.github.io/prophet/docs/quick_start.html#python-api)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQFdL-M305BW"
      },
      "outputs": [],
      "source": [
        "#pip install pystan==2.19.1.1\n",
        "#pip install prophet\n",
        "from prophet import Prophet\n",
        "\n",
        "model = Prophet(yearly_seasonality=True)\n",
        "\n",
        "# model.fit(df_forecast.iloc[-17472:])\n",
        "model.fit(df_forecast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8dX-R3d05BW"
      },
      "source": [
        "For a complete list of parameters and instructions on how to include the holyday component, see the [documentation](https://facebook.github.io/prophet/docs/quick_start.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Boj-w4Te05BW"
      },
      "outputs": [],
      "source": [
        "num_future = 168\n",
        "\n",
        "future = model.make_future_dataframe(periods=num_future, freq='h')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRSVpMBs05BW"
      },
      "outputs": [],
      "source": [
        "forecast = model.predict(future)\n",
        "\n",
        "# Apply floor operator because number cannot be negative\n",
        "forecast['yhat'] = forecast['yhat'].apply(lambda x: np.clip(x,0,None))\n",
        "forecast['yhat_lower'] = forecast['yhat_lower'].apply(lambda x: np.clip(x,0,None))\n",
        "forecast['yhat_upper'] = forecast['yhat_upper'].apply(lambda x: np.clip(x,0,None))\n",
        "\n",
        "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJu_IA_q05BW"
      },
      "source": [
        "In case you have not installed `plotly` or it does not work, you can also display the result by using `Matplotlib`:<br><br>\n",
        "`fig1 = model.plot(forecast)`<br>\n",
        "`fig2 = model.plot_components(forecast)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OHcRMMP05BW"
      },
      "outputs": [],
      "source": [
        "# pip install ipywidgets\n",
        "# pip install plotly\n",
        "from prophet.plot import plot_plotly, plot_components_plotly\n",
        "plot_plotly(model, forecast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9ZSic3W05BX"
      },
      "source": [
        "The advantage of splitting the model into different components is that the contribution of each component can be analyse separately.\n",
        "This allows conclusions to be drawn about the development of the time series.\n",
        "See below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zV4PGJqe05BX"
      },
      "outputs": [],
      "source": [
        "plot_components_plotly(model, forecast)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6.9 ('venv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "747131c1b9b3e4ac9b17fc6ae0b784c32121a8c91b4324a16e14e5d85f0a17bd"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}